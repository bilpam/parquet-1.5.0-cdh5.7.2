From 9d6357972b7aae8aacb65de57551f914df932c25 Mon Sep 17 00:00:00 2001
From: Nezih Yigitbasi <nyigitbasi@netflix.com>
Date: Tue, 28 Jul 2015 14:55:14 -0700
Subject: [PATCH 156/176] PARQUET-342: Updates to be Java 6 compatible

Author: Nezih Yigitbasi <nyigitbasi@netflix.com>

Closes #248 from nezihyigitbasi/java6-fixes and squashes the following commits:

2ab2598 [Nezih Yigitbasi] Updates to be Java 6 compatible

Conflicts:
	parquet-hadoop/src/test/java/parquet/hadoop/TestInputOutputFormatWithPadding.java
Resolution:
    Fix package names.
---
 parquet-common/src/main/java/parquet/Files.java    |   51 ++++++++++++++++++++
 .../src/main/java/parquet/SemanticVersion.java     |   16 ++++--
 .../hadoop/TestInputOutputFormatWithPadding.java   |   11 ++---
 .../hadoop/example/TestInputOutputFormat.java      |   16 +++---
 4 files changed, 76 insertions(+), 18 deletions(-)
 create mode 100644 parquet-common/src/main/java/parquet/Files.java

diff --git a/parquet-common/src/main/java/parquet/Files.java b/parquet-common/src/main/java/parquet/Files.java
new file mode 100644
index 0000000..a63b033
--- /dev/null
+++ b/parquet-common/src/main/java/parquet/Files.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package parquet;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.nio.charset.Charset;
+import java.util.ArrayList;
+import java.util.List;
+
+//TODO: Use java.nio.file.Files when Parquet is updated to Java 7
+public final class Files {
+  private Files() { }
+
+  public static List<String> readAllLines(File file, Charset charset) throws IOException {
+    BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(file), charset));
+    try {
+      List<String> result = new ArrayList<String>();
+      for (;;) {
+        String line = reader.readLine();
+        if (line == null)
+          break;
+        result.add(line);
+      }
+      return result;
+    }
+    finally {
+      reader.close();
+    }
+  }
+}
diff --git a/parquet-common/src/main/java/parquet/SemanticVersion.java b/parquet-common/src/main/java/parquet/SemanticVersion.java
index 7028778..316e1cb 100644
--- a/parquet-common/src/main/java/parquet/SemanticVersion.java
+++ b/parquet-common/src/main/java/parquet/SemanticVersion.java
@@ -96,22 +96,30 @@ public final class SemanticVersion implements Comparable<SemanticVersion> {
   public int compareTo(SemanticVersion o) {
     int cmp;
 
-    cmp = Integer.compare(major, o.major);
+    cmp = compareIntegers(major, o.major);
     if (cmp != 0) {
       return cmp;
     }
 
-    cmp = Integer.compare(minor, o.minor);
+    cmp = compareIntegers(minor, o.minor);
     if (cmp != 0) {
       return cmp;
     }
 
-    cmp = Integer.compare(patch, o.patch);
+    cmp = compareIntegers(patch, o.patch);
     if (cmp != 0) {
       return cmp;
     }
 
-    return Boolean.compare(o.prerelease, prerelease);
+    return compareBooleans(o.prerelease, prerelease);
+  }
+
+  int compareIntegers(int x, int y) {
+    return (x < y) ? -1 : ((x == y) ? 0 : 1);
+  }
+
+  int compareBooleans(boolean x, boolean y) {
+    return (x == y) ? 0 : (x ? 1 : -1);
   }
 
   @Override
diff --git a/parquet-hadoop/src/test/java/parquet/hadoop/TestInputOutputFormatWithPadding.java b/parquet-hadoop/src/test/java/parquet/hadoop/TestInputOutputFormatWithPadding.java
index 2f8e2a4..325aa6c 100644
--- a/parquet-hadoop/src/test/java/parquet/hadoop/TestInputOutputFormatWithPadding.java
+++ b/parquet-hadoop/src/test/java/parquet/hadoop/TestInputOutputFormatWithPadding.java
@@ -19,7 +19,6 @@
 package parquet.hadoop;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
@@ -28,6 +27,7 @@ import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
+import parquet.Files;
 import parquet.example.data.Group;
 import parquet.example.data.simple.SimpleGroupFactory;
 import parquet.format.converter.ParquetMetadataConverter;
@@ -42,13 +42,11 @@ import org.junit.Assert;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
+
 import java.io.File;
 import java.io.FileOutputStream;
 import java.io.IOException;
-import java.net.URI;
 import java.nio.charset.Charset;
-import java.nio.file.Files;
-import java.nio.file.Paths;
 import java.util.UUID;
 
 import static java.lang.Thread.sleep;
@@ -66,6 +64,8 @@ public class TestInputOutputFormatWithPadding {
       .required(BINARY).as(UTF8).named("char")
       .named("FormatTestObject");
 
+  private static final Charset UTF_8 = Charset.forName("UTF-8");
+
   /**
    * ParquetInputFormat that will not split the input file (easier validation)
    */
@@ -179,8 +179,7 @@ public class TestInputOutputFormatWithPadding {
     Assert.assertNotNull("Should find a data file", dataFile);
 
     StringBuilder contentBuilder = new StringBuilder();
-    for (String line : Files.readAllLines(
-        Paths.get(dataFile.toURI()), Charset.forName("UTF-8"))) {
+    for (String line : Files.readAllLines(dataFile, UTF_8)) {
       contentBuilder.append(line);
     }
     String reconstructed = contentBuilder.toString();
diff --git a/parquet-hadoop/src/test/java/parquet/hadoop/example/TestInputOutputFormat.java b/parquet-hadoop/src/test/java/parquet/hadoop/example/TestInputOutputFormat.java
index a5c5128..0c5e23c 100644
--- a/parquet-hadoop/src/test/java/parquet/hadoop/example/TestInputOutputFormat.java
+++ b/parquet-hadoop/src/test/java/parquet/hadoop/example/TestInputOutputFormat.java
@@ -25,15 +25,12 @@ import java.io.BufferedReader;
 import java.io.File;
 import java.io.FileReader;
 import java.io.IOException;
-import java.lang.reflect.Method;
 import java.nio.charset.Charset;
-import java.nio.file.Files;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -44,14 +41,16 @@ import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
-import org.apache.parquet.Strings;
-import org.apache.parquet.filter2.predicate.FilterApi;
+import parquet.Files;
+import parquet.Strings;
+import parquet.filter2.predicate.FilterApi;
 import org.junit.Before;
 import org.junit.Test;
 
 import parquet.Log;
 import parquet.example.data.Group;
 import parquet.example.data.simple.SimpleGroupFactory;
+import parquet.hadoop.ParquetInputFormat;
 import parquet.hadoop.api.ReadSupport;
 import parquet.hadoop.metadata.CompressionCodecName;
 import parquet.hadoop.util.ContextUtil;
@@ -59,6 +58,7 @@ import parquet.schema.MessageTypeParser;
 
 public class TestInputOutputFormat {
   private static final Log LOG = Log.getLog(TestInputOutputFormat.class);
+  private static final Charset UTF_8 = Charset.forName("UTF-8");
   final Path parquetPath = new Path("target/test/example/TestInputOutputFormat/parquet");
   final Path inputPath = new Path("src/test/java/parquet/hadoop/example/TestInputOutputFormat.java");
   final Path outputPath = new Path("target/test/example/TestInputOutputFormat/out");
@@ -213,7 +213,7 @@ public class TestInputOutputFormat {
       put(ParquetInputFormat.FILTER_PREDICATE, fpString);
     }});
 
-    List<String> lines = Files.readAllLines(new File(outputPath.toString(), "part-m-00000").toPath(), Charset.forName("UTF-8"));
+    List<String> lines = Files.readAllLines(new File(outputPath.toString(), "part-m-00000"), UTF_8);
     assertTrue(lines.isEmpty());
   }
 
@@ -231,7 +231,7 @@ public class TestInputOutputFormat {
       put(ParquetInputFormat.FILTER_PREDICATE, fpString);
     }});
 
-    List<String> expected = Files.readAllLines(new File(inputPath.toString()).toPath(), Charset.forName("UTF-8"));
+    List<String> expected = Files.readAllLines(new File(inputPath.toString()), UTF_8);
 
     // grab the lines that contain the first 500 characters (including the rest of the line past 500 characters)
     int size = 0;
@@ -248,7 +248,7 @@ public class TestInputOutputFormat {
     }
 
     // put the output back into it's original format (remove the character counts / tabs)
-    List<String> found = Files.readAllLines(new File(outputPath.toString(), "part-m-00000").toPath(), Charset.forName("UTF-8"));
+    List<String> found = Files.readAllLines(new File(outputPath.toString(), "part-m-00000"), UTF_8);
     StringBuilder sbFound = new StringBuilder();
     for (String line : found) {
       sbFound.append(line.split("\t", -1)[1]);
-- 
1.7.9.5

